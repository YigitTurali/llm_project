{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehmet/miniconda3/envs/llm_project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchtune.models.llama3_2 import llama3_2_1b, lora_llama3_2_1b\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Llama3.2 1B without any LoRA layers\n",
    "base_model = llama3_2_1b()\n",
    "lora_model = lora_llama3_2_1b(lora_attn_modules=[\"q_proj\",\"v_proj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "  (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "  (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (pos_embeddings): Llama3ScaledRoPE()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.layers[0].attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (q_proj): LoRALinear(\n",
       "    (dropout): Identity()\n",
       "    (lora_a): Linear(in_features=2048, out_features=8, bias=False)\n",
       "    (lora_b): Linear(in_features=8, out_features=2048, bias=False)\n",
       "  )\n",
       "  (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "  (v_proj): LoRALinear(\n",
       "    (dropout): Identity()\n",
       "    (lora_a): Linear(in_features=2048, out_features=8, bias=False)\n",
       "    (lora_b): Linear(in_features=8, out_features=512, bias=False)\n",
       "  )\n",
       "  (output_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  (pos_embeddings): Llama3ScaledRoPE()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.layers[0].attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json.load(open('/home/mehmet/codebase/llm_project/data/CustomeData.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in dataset:\n",
    "    if 'instruction' in element: \n",
    "        del element['instruction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/mehmet/codebase/llm_project/data/CustomeDataNoInstructions.json','w', encoding='utf-8') as f:\n",
    "    json.dump(dataset, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['layers.0.attn.q_proj.lora_a.weight', 'layers.0.attn.q_proj.lora_b.weight', 'layers.0.attn.v_proj.lora_a.weight', 'layers.0.attn.v_proj.lora_b.weight', 'layers.1.attn.q_proj.lora_a.weight', 'layers.1.attn.q_proj.lora_b.weight', 'layers.1.attn.v_proj.lora_a.weight', 'layers.1.attn.v_proj.lora_b.weight', 'layers.2.attn.q_proj.lora_a.weight', 'layers.2.attn.q_proj.lora_b.weight', 'layers.2.attn.v_proj.lora_a.weight', 'layers.2.attn.v_proj.lora_b.weight', 'layers.3.attn.q_proj.lora_a.weight', 'layers.3.attn.q_proj.lora_b.weight', 'layers.3.attn.v_proj.lora_a.weight', 'layers.3.attn.v_proj.lora_b.weight', 'layers.4.attn.q_proj.lora_a.weight', 'layers.4.attn.q_proj.lora_b.weight', 'layers.4.attn.v_proj.lora_a.weight', 'layers.4.attn.v_proj.lora_b.weight', 'layers.5.attn.q_proj.lora_a.weight', 'layers.5.attn.q_proj.lora_b.weight', 'layers.5.attn.v_proj.lora_a.weight', 'layers.5.attn.v_proj.lora_b.weight', 'layers.6.attn.q_proj.lora_a.weight', 'layers.6.attn.q_proj.lora_b.weight', 'layers.6.attn.v_proj.lora_a.weight', 'layers.6.attn.v_proj.lora_b.weight', 'layers.7.attn.q_proj.lora_a.weight', 'layers.7.attn.q_proj.lora_b.weight', 'layers.7.attn.v_proj.lora_a.weight', 'layers.7.attn.v_proj.lora_b.weight', 'layers.8.attn.q_proj.lora_a.weight', 'layers.8.attn.q_proj.lora_b.weight', 'layers.8.attn.v_proj.lora_a.weight', 'layers.8.attn.v_proj.lora_b.weight', 'layers.9.attn.q_proj.lora_a.weight', 'layers.9.attn.q_proj.lora_b.weight', 'layers.9.attn.v_proj.lora_a.weight', 'layers.9.attn.v_proj.lora_b.weight', 'layers.10.attn.q_proj.lora_a.weight', 'layers.10.attn.q_proj.lora_b.weight', 'layers.10.attn.v_proj.lora_a.weight', 'layers.10.attn.v_proj.lora_b.weight', 'layers.11.attn.q_proj.lora_a.weight', 'layers.11.attn.q_proj.lora_b.weight', 'layers.11.attn.v_proj.lora_a.weight', 'layers.11.attn.v_proj.lora_b.weight', 'layers.12.attn.q_proj.lora_a.weight', 'layers.12.attn.q_proj.lora_b.weight', 'layers.12.attn.v_proj.lora_a.weight', 'layers.12.attn.v_proj.lora_b.weight', 'layers.13.attn.q_proj.lora_a.weight', 'layers.13.attn.q_proj.lora_b.weight', 'layers.13.attn.v_proj.lora_a.weight', 'layers.13.attn.v_proj.lora_b.weight', 'layers.14.attn.q_proj.lora_a.weight', 'layers.14.attn.q_proj.lora_b.weight', 'layers.14.attn.v_proj.lora_a.weight', 'layers.14.attn.v_proj.lora_b.weight', 'layers.15.attn.q_proj.lora_a.weight', 'layers.15.attn.q_proj.lora_b.weight', 'layers.15.attn.v_proj.lora_a.weight', 'layers.15.attn.v_proj.lora_b.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.load_state_dict(base_model.state_dict(),strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  1236666368 total params,\n",
      "  851968\" trainable params,\n",
      "  0.07% of all params are trainable.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from torchtune.modules.peft import get_adapter_params, set_trainable_params\n",
    "\n",
    "#Fetch all paramas from the model that are associated with LoRA.\n",
    "lora_params = get_adapter_params(lora_model)\n",
    "\n",
    "#Set requires_grad= True for LoRA and False for all others.\n",
    "set_trainable_params(lora_model,lora_params)\n",
    "\n",
    "# Print the total number of parameters\n",
    "total_params = sum([p.numel() for p in lora_model.parameters()])\n",
    "trainable_params = sum([p.numel() for p in lora_model.parameters() if p.requires_grad])\n",
    "print(\n",
    "  f\"\"\"\n",
    "  {total_params} total params,\n",
    "  {trainable_params}\" trainable params,\n",
    "  {(100.0 * trainable_params / total_params):.2f}% of all params are trainable.\n",
    "  \"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
